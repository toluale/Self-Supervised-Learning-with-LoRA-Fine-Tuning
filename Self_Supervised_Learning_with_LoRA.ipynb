{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXeXdVT2ByJ3"
   },
   "source": [
    "This Notebook is Authored by: **Tolulope Ale** for iHARP AI Workshop Series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4gKbOfcR3Kt"
   },
   "source": [
    "# Self-Supervised Learning with LoRA Fine-Tuning\n",
    "\n",
    "This notebook demonstrates a complete pipeline for:\n",
    "1. **Self-Supervised Learning (SSL)** - Pre-training a model using denoising autoencoder\n",
    "2. **LoRA (Low-Rank Adaptation)** - Parameter-efficient fine-tuning\n",
    "3. **Evaluation** - Comparing SSL+LoRA against baseline models\n",
    "\n",
    "## Approach Overview\n",
    "- **Phase 1**: SSL pre-training on CNN/DailyMail dataset using denoising autoencoder\n",
    "- **Phase 2**: LoRA fine-tuning on DialogSum dataset with minimal labeled data\n",
    "- **Phase 3**: Evaluation using ROUGE, BLEU, and BERTScore metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WxzN6XTr7NY",
    "outputId": "ad5512b4-b366-48a5-d21e-247a1fe45644"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AVEpdFFScWn"
   },
   "source": [
    "## Load Base Model\n",
    "\n",
    "We load **FLAN-T5 Base**, a text-to-text transformer model from Google that has been instruction-tuned. This will serve as our starting point for both SSL pre-training and subsequent fine-tuning.\n",
    "\n",
    "**FLAN-T5**: Fine-tuned Language Net - Text-to-Text Transfer Transformer (5th generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382,
     "referenced_widgets": [
      "2c3c1d7619da4f5f8dd0dd32a3c6e74b",
      "e45da24436194feaa47623e07865e850",
      "64ef6d4bbfc646419a4b9538bea3feb2",
      "d8bb30665f41432bace58b0f715791ac",
      "ef06f2b1140c42d8a22db3e84c95a7e7",
      "22f8f34ee03248c89f332d184526437c",
      "77965204417c470db8b284ce051fdb9d",
      "2fefcef156134ca3a60321d82076c8ad",
      "1a0bbe1fcd234b2eaf8366631c159417",
      "b7de4b73000845789327a96a82182473",
      "3a4d6947dfc542f69734d109fcebffad",
      "632e27f056ff4b9fb1ef91267e5c5eee",
      "e3f4dbfd03324971bb8b8a96c8756ccd",
      "85f6bf9513fe457a8aa858c9ea2a6b5f",
      "030eb2cbd0cd46f2ad237698baa62a47",
      "cbe743410b9a4dd5b940cf8c78100e70",
      "93e70ccf483644daa70f14cdd771beb6",
      "f3646b8c8933403288e3fcb4fe58ec49",
      "bf487bec426549d4b0efbf6b5a594641",
      "cf0f0b7d50714302be31ad078b75d4a8",
      "8dc0f3ed281b4ff3a68873568b8582aa",
      "fa82372f57d84d57bc06cc182372e651",
      "d5bc9050117f48f787a99a4aa52f7edd",
      "36a814b6bdf04534892a142d22fc8fbe",
      "57107c5ee8024f3196ccf6cf2fd84b30",
      "8652d2ae2a86469f8d4b00ba82de4d8f",
      "6896a57c4d3a4f7aa983a4fa54c0cb50",
      "801bff7efa1a4b19aabde44b4fa815af",
      "461739313c354d1892967c77e070adfc",
      "0c6ea81d64564191a7c0a78e19c1d13a",
      "04d3ae90aab54ce0a09fae6a76078f78",
      "d4fadda4661640ba9fa38ea20e7e5669",
      "570c3078212d407a81654766e122ea93",
      "1c5050ddac354fadbd2b9464ddde1842",
      "4241178f40244151921fbfac29cd3002",
      "589b13ad0c5f4291bee6555a795e14e3",
      "b54161f18dc14afa9e444328d2d9d1e4",
      "a1765b2adbcf4d73ab8088182fe04c7a",
      "2ce492f070f147e08e0a42f1253b7b6b",
      "fe2e027f70d14ec8a6646979d7646acc",
      "0ea258697fac44418e97e5009d4d5867",
      "77ab99ff23004cb3bdb6b06b0c7c2ddf",
      "414c6d315bcf473781affd7a0c515191",
      "a1da31985e864f0d8d3228f31bf8f9e2",
      "c975af2625fd4adeb2d8140fc95e3510",
      "91f0f0f9160d4b339e8cdcba0af37b7c",
      "5236b460caa6452e884206fa9587c63b",
      "881d7085a17449ad9c84ef2909cabace",
      "823f305f40d84f5a9e3a4d3917b04d26",
      "d1a7aeced302405aa89d28794abf7f39",
      "a21af99135c74c1da73ff7702c1239e3",
      "4ee8fb1c7d184a7ca3c62c24c897d849",
      "09a21d6b75d349a983a27b25e5e4d6f4",
      "3a97470c60ca40d19cbc8ec58a9b7ef9",
      "b87205ff9fce4173aa94e528e74485b0",
      "964e1ab0d574407691105898bef4dce9",
      "bdd65d5963bf4713a2e6694a71b42e4f",
      "97e010b11003421ea23a0071eb95934d",
      "bde8bd10cdde42809cd8716bc446602f",
      "a914af3a22f74d5d8e5c5f2c366c07e8",
      "e175f436cd7e418db949e5920028aab4",
      "e7d26e2f235b4b67ad56577f0a177bcf",
      "48864ae3c9c442a680ce4dfaa24e4128",
      "484dfbadd34b48a79bd85de82e6fe4af",
      "35cd1fc705644714a97f1f746670f966",
      "a08e33d781e0495b9972450fb6712596",
      "a1eeb37454a346fdb06ee9090ddcb8fe",
      "345186b9b5cb421fa08345a30233a2af",
      "cc755c4b6b62411393a060b0150afa56",
      "401139041530444882320f336da0e4ba",
      "6d12e5bc95a145abb12a594fd2395375",
      "17e4af70979e426488803302ac7167c9",
      "17e949117b9e429e82baf358459f1003",
      "110b87bafc234601a207076934d05a25",
      "f466e479fe0a443da5fe74ecafe4ae3b",
      "194b7b06016d401f8c3c5eee3a9dee69",
      "f4abc6ea1ffd4aa5a008563b10fa6e55"
     ]
    },
    "id": "_XtWTycHrS7j",
    "outputId": "c77e3941-58d2-4415-f95b-5269c0ad74e6"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Loading FLAN-T5 Base model - text-summarization model\n",
    "\n",
    "model_name= 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) #FLAN-T5 tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9D9itu3YStpQ"
   },
   "source": [
    "## Load SSL Pre-training Dataset\n",
    "\n",
    "We load the **CNN/DailyMail** dataset which contains news articles and summaries. This large dataset (287k+ training examples) will be used for self-supervised learning.\n",
    "\n",
    "**Why CNN/DailyMail?** It's a large, diverse corpus perfect for SSL pre-training where we don't need high-quality labels - we'll create our own training signal through data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305,
     "referenced_widgets": [
      "7580464315764a7184eeaabd2c358088",
      "c76e655dbc244d7398623e22fc392eb1",
      "fad35e0db0ca4c9fb1e5357beda7bfbb",
      "b33e2f7a29b74bb5b3b3367c33d40546",
      "0ceafb76307b4b72a31043ef29f3e31f",
      "07d32160a39f419c8d319d9ccc2d1f8b",
      "fcc38ee065cf4163898c9e8a63354b18",
      "52a92241f5364bfc99597a4f2901d400",
      "df76e58b2fc440bdb3a68b61884184e2",
      "f0c77aa4d9914f7d9290d1ab4af7713c",
      "e121b25a120d43578a2d57e5e434c6a9",
      "2c5f3567e72a4a04aff7c3c5933ffe3f",
      "410f39001f1e4b7c8d5575145af85afa",
      "5d205ab839044837877e65d62f514493",
      "460a511bf50d45d1871b4668a9c90c83",
      "a4bbd290827e4ebdabf50eb9af7a81fc",
      "720d8281e9b24807bc07bd7292dfd9b0",
      "8e8a5181fcf6470f806698f384fa3352",
      "f5ec81c2dc0b479c8304f7018af9ecee",
      "14c16fe8ead14e9fb238cab16a13b343",
      "7d71536d7954444cbafa08010932732f",
      "2c07cd1ebed34dd2af871c676a0c6e7f",
      "d64844b95d6d43c1842c40f4e0673e6a",
      "d2a1d218dd734d8587f6d33783500598",
      "e5ba326106d44f8387e841e516ef442c",
      "3e5622c361c042c2881a8206ec84f551",
      "a38a773c664941e5ad438e4bda9886dd",
      "b5c12a75485240b29c872ce4041b2136",
      "262d2633c35144afb2d059b1ad11d5fb",
      "26be13642c7147f884809a43f69e2f10",
      "5be060d33b3d49489cc46dab9522c1f3",
      "f1d92c5db3f8498993ce28e05dabda75",
      "e983415ae09241398312c64a422d0671",
      "8bc7ed1577ed49b3a856460e0a907adb",
      "3bb7dfde630345cdbd082c3cd368cd07",
      "ca69f9c08ef841a9aa4b950a09b6eca9",
      "f2c9b2e957714c84b6338f6f2a975e48",
      "9fe941564e04474b9d3a36f8bffc13d5",
      "0111c086dfdf44dd9f1a436f6e5a0d40",
      "d0874ceb5d104c88b7710f90c87de055",
      "f377d92917fd4da496b3ac2fe118d641",
      "407d0947af6642acadb5c1591a1038ba",
      "198cd59bdc414094bde7daab56c0ff47",
      "45a9b839834d4f2886de00ab289f8cd3",
      "2b49677c377c4f2ba5b2c5383f70a887",
      "6b8a66b613e34a93ac18fe195111d9c0",
      "ebe10790479847378a4d9e7dde081921",
      "32888bfcd06249fb8091280264e31417",
      "f9a30c4e45cf45fd9e3acc09e8d8af41",
      "9de763e7499048d9a846a8ee3f16c58b",
      "cb20e0601bfe4994aa7736ede3997242",
      "81d1f3967e06455395d406fdd63561ec",
      "51477d7de74c4056aa4ca4e6b534fcef",
      "ad9650ab8a5048b0890f1f952ba0f235",
      "3063cf3b23324f0d82e9974ecb5da2ab",
      "dbd66a453389489a8a904e89a765f642",
      "1181c9d6714d48339e97e3548430c630",
      "285b663d3ede427388a0a38f0a1777b9",
      "dbfe51fc96ae4e8ab48cfc0b1bf0cccf",
      "6ad5f67cc21f4488845c7e6463d9ecc7",
      "ca9da3f015494ff480855e728caafe5c",
      "cdb697f15cf94f0ca5a940239d1d4989",
      "9382c1781727498c98b3ee86d38603d9",
      "74c2299368004fa7b53ba6df9f5ba42c",
      "e97400ac808b4251ace983299a3f78c5",
      "43a256c1b5524606a5f70c05b72768d3",
      "bf94fd0703b448e6adb6b9cd3d87dd99",
      "63611ff005f84c2d8b1df01a4fa256a5",
      "6f57a1fced43419b96d1c14fdf2273d3",
      "f99b0f05d1ed41e3a9dce7b6817c460e",
      "dc1ab2b060e34e6d9ff236304b566686",
      "213b05c1ed494572ad13b8edd991c8b4",
      "92760abe8b134c878952cd9f217d408b",
      "d07589ad73d24a0285de6f472365ef70",
      "647b8eda04b34bac80824a29c17384ff",
      "80ae419dd4b941a8b4204697f696f979",
      "e037818927c147f7966680dfc2466271",
      "69462aee7d334094b3770aff55cec735",
      "46935f4fed8841759611780acb5a1ce6",
      "59a90b922bc74e1c8af65fb6ea725357",
      "bc25d01d9d544b279abb05f7caff4642",
      "c028c95303a547a2af3e6a744d6b759a",
      "951fe8676d294f1ba793bcc8cca5b6dd",
      "26c2788cda2640c2875212285f40e3ca",
      "64ebf635c4bd47ae84bb461f06ff7717",
      "1f6a1949578d4788a81072d46398b375",
      "799d4f381ccf43d79cc0449ba426b819",
      "cec525d698994f4c864db1c676b01905",
      "6936db72c4e04382878749083d8849ca",
      "00ee6bd1ce8a4d189c2fe6e3f456bf88",
      "4a0fce1975694f8bb86e4456e6740d8c",
      "80439df19b6d43ed83a43243e6fcbbb2",
      "36e31ba418c041fcb559cfaddfd2d7f9",
      "bf143916511c4e099a84391e00ba7842",
      "4d10a3f53f0240c8b5ebdab60e12f81f",
      "f5c57411f8ef490f8e8574ff7c8ca2a0",
      "741c4a1de68b4dd8b897a4ab598bdad1",
      "ae53ed7393634b6ba013035ece2b997d",
      "ebb2f5cbea85440aa99e75c55660e447"
     ]
    },
    "id": "nip7crVtr5nv",
    "outputId": "1500926d-0ab4-499a-b2fd-c71cbaca168b"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "#Text Summarization\n",
    "ssl_dataset_name = \"abisee/cnn_dailymail\" #\"knkarthick/dialogsum\" #loading the dataset from huggingface\n",
    "ssl_dataset = load_dataset(ssl_dataset_name, '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ENva7uZVusY3",
    "outputId": "1f390f81-a72f-45ed-8068-1d79eafc47c3"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder #inspection of dataset features\n",
    "\n",
    "data_builder = load_dataset_builder(ssl_dataset_name, '3.0.0')\n",
    "\n",
    "print(data_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33FhRFTIvYfY",
    "outputId": "4163ea11-d092-4cf2-8b2a-8d772cf0017b"
   },
   "outputs": [],
   "source": [
    "ssl_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21Es32FtTW8H"
   },
   "source": [
    "Examine a few examples from the dataset to understand the article structure and baseline summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sq654CZhvmnn",
    "outputId": "de428181-3c8e-44a9-97e9-811276291546"
   },
   "outputs": [],
   "source": [
    "#Loading some sample article with their baseline summaries\n",
    "dialogues = [10, 505, 800, 750]\n",
    "\n",
    "dash_line = '*'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(dialogues):\n",
    "    print(dash_line)\n",
    "    print('Sample ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT ARTICLE:')\n",
    "    print(ssl_dataset['test'][index]['article'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE SUMMARY:')\n",
    "    print(ssl_dataset['test'][index]['highlights'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8elXkIEBTnQ6"
   },
   "source": [
    "## Baseline Model Performance\n",
    "\n",
    "Test the **out-of-the-box** FLAN-T5 model without any training. This establishes our baseline - what the model can do with zero task-specific training.\n",
    "\n",
    "**Note**: No prompt engineering is used here - we're testing raw model capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OT88HNKE7yhQ",
    "outputId": "00d3584c-3bc9-4bc7-ee94-9adfacf7c6ba"
   },
   "outputs": [],
   "source": [
    "# Base LLM summarization without prompt engineering\n",
    "for i, index in enumerate(dialogues):\n",
    "    article = ssl_dataset['test'][index]['article']\n",
    "    summary = ssl_dataset['test'][index]['highlights']\n",
    "\n",
    "    inputs = tokenizer(article, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=500,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Sample ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT ARTICLE:\\n{article}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG8fU-iyqkwA"
   },
   "source": [
    "---\n",
    "\n",
    "# Self-Supervised Learning (SSL)  \n",
    "\n",
    "## Pre-text Task\n",
    "\n",
    "We'll pre-train the model using a **denoising autoencoder** approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVIy1B2fqlJ6"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "#import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78xwz_NHUYxL"
   },
   "source": [
    "## Reload Model with Optimized Settings\n",
    "\n",
    "We reload the FLAN-T5 model with **bfloat16** precision to reduce memory usage and speed up training while maintaining model quality.\n",
    "\n",
    "**bfloat16**: Brain Floating Point 16-bit - A reduced precision format that's more stable than float16 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UC9tD8pDq5jK"
   },
   "outputs": [],
   "source": [
    "# We will reload the FLAN-T5 base model but the small version due to compute\n",
    "# by setting torch_dtype=torch.bfloat16\n",
    "\n",
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, dtype=torch.bfloat16)\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jvRnOKTUvsY"
   },
   "source": [
    "Create a utility function to examine how many parameters are trainable vs frozen. This is crucial for understanding computational requirements and training dynamics.\n",
    "\n",
    "**Key Insight**: FLAN-T5 Base has ~248M parameters - training all of them requires significant compute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E88E9d4RrBjL",
    "outputId": "0953d824-90d8-4754-ba88-39d9000e518e"
   },
   "outputs": [],
   "source": [
    "# Examining the number of trainable model parameters\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-F04IjuU5yx"
   },
   "source": [
    "## Create Denoising Autoencoder Setup\n",
    "\n",
    "**Self-Supervised Learning** core implementation!\n",
    "\n",
    "### What is a Denoising Autoencoder?\n",
    "A denoising autoencoder learns to reconstruct clean input from corrupted input. The model learns robust representations by recovering original data from noisy versions.\n",
    "\n",
    "### Our Approach:\n",
    "- **Input**: Article with 20% of words randomly dropped (noisy)\n",
    "- **Target**: Original clean article\n",
    "- **Learning Signal**: Model learns to predict missing words, understanding context and semantics\n",
    "\n",
    "This creates a training signal from **unlabeled data** - no human annotations needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182,
     "referenced_widgets": [
      "e49a33472a4c401d8ea99ca962c829ba",
      "48585cd980594b10ab7fe2f8ab2d2484",
      "428ba2378b3049528fb187acb326fb97",
      "1d389b7661484d7db7ac7187e9a3713b",
      "b68d4c154133468db8b777a97a049f25",
      "dda4520350cb44899b8d6db27f158f1e",
      "d194e41a804a474b971cfb07da09168f",
      "416ae789e71a4a2f8398c90a6f271d7a",
      "d72a6d817f7447f49a6b42267bc93646",
      "aeda111753534f75a30488e2102b73e2",
      "4d75200c3cf74704b906b5d48f20d348",
      "456f7c3d6b1b4d258cb8073a0cf1471a",
      "c3ea5b9f183d4810adcf8a1ad3002531",
      "9ecc6a2b3842422f8d5a0900b68a7623",
      "71a9813265334113bcad050faaf119f8",
      "8d92a7d1aec74d98a6be04e129f722d0",
      "6bb18129d77d4690bff9541e5d2b4b40",
      "62faddbba41b451797ebd534f8f4486b",
      "cb84306199414382a471b50717791e90",
      "1723c37cdb3b491ca67e38187b9579c5",
      "a124aa65b88742bf827a609fff04f5ed",
      "d704df90b99f4cd989ad065517e68fd2",
      "f982f82725984ec19feff75aeb71f1fa",
      "ab55dcef18b04f6da545958c1336e92c",
      "4707840f2e2148428cb3f35457764398",
      "ad8095adcc434310a775a0deb59db4dc",
      "57107d2eb4d94fb08b09e5cdae24775b",
      "930739fcf5ba49c1943f30aa5dcfa917",
      "26f13ef23e0f487da52dd798eb2bc37a",
      "fea9e50cc8164df889bdcd4ecebbb27a",
      "4e8765abc4674703abd457df05f9cbdb",
      "50183a417e3e4b60ab1cebc71815f93d",
      "d5da062dc505434daaa91b890bca162b"
     ]
    },
    "id": "FFIZFxglrMFa",
    "outputId": "903b67ea-bbae-4166-f691-6dd6dde1077a"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create noise function for denoising autoencoder\n",
    "def add_noise(text, drop_prob=0.2):\n",
    "    \"\"\"Randomly drop words from text to create noisy input\"\"\"\n",
    "    words = text.split()\n",
    "    noisy = [w for w in words if random.random() > drop_prob]\n",
    "    return ' '.join(noisy) if noisy else text  # Return original if all dropped\n",
    "\n",
    "# SSL tokenization function\n",
    "def tokenize_function_ssl(example):\n",
    "    \"\"\"Create noisy input and clean target for denoising task\"\"\"\n",
    "    # Apply noise to article\n",
    "    noisy_article = [add_noise(d) for d in example[\"article\"]]\n",
    "\n",
    "    # Noisy article as input\n",
    "    example['input_ids'] = tokenizer(\n",
    "        noisy_article,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    # Clean dialogue as target (labels)\n",
    "    example['labels'] = tokenizer(\n",
    "        example[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    "# Apply SSL tokenization to dataset\n",
    "print(\"Applying SSL tokenization...\")\n",
    "ssl_datasets = ssl_dataset.map(tokenize_function_ssl, batched=True)\n",
    "ssl_datasets = ssl_datasets.remove_columns(['id', 'article', 'highlights'])\n",
    "\n",
    "print(f\"SSL Dataset shapes:\")\n",
    "print(f\"Training: {ssl_datasets['train'].shape}\")\n",
    "print(f\"Validation: {ssl_datasets['validation'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRTzvGkAVSpV"
   },
   "source": [
    "## Verify that we're creating meaningful corruption (not too much, not too little)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruXZQmq6I8_U",
    "outputId": "fbc70ba5-5629-4a16-ae9c-90ca17898a76"
   },
   "outputs": [],
   "source": [
    "# Displaying samples of noisy vs clean data\n",
    "print(\"=\"*100)\n",
    "print(\"EXAMPLES OF NOISY DATA vs CLEAN DATA\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "num_examples = 3\n",
    "sample_indices = [0, 50, 100]  # Show different samples\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    clean_text = ssl_dataset['train'][idx]['article']\n",
    "    noisy_text = add_noise(clean_text, drop_prob=0.2)\n",
    "\n",
    "    # Calculate how many words were dropped\n",
    "    clean_words = len(clean_text.split())\n",
    "    noisy_words = len(noisy_text.split())\n",
    "    dropped_words = clean_words - noisy_words\n",
    "\n",
    "    print(f\"\\n{'*'*100}\")\n",
    "    print(f\"SAMPLE {i+1} (Index {idx})\")\n",
    "    print(f\"{'*'*100}\")\n",
    "    print(f\"\\nCLEAN TEXT ({clean_words} words):\")\n",
    "    print(f\"{clean_text[:500]}...\")  # Show first 500 chars\n",
    "    print(f\"\\n{'-'*100}\")\n",
    "    print(f\"\\nNOISY TEXT ({noisy_words} words, {dropped_words} words dropped, {dropped_words/clean_words*100:.1f}% reduction):\")\n",
    "    print(f\"{noisy_text[:500]}...\")  # Show first 500 chars\n",
    "    print(f\"\\n{'='*100}\")\n",
    "\n",
    "print(f\"\\n Noise function randomly drops ~20% of words to create denoising task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5erqqnURViBq"
   },
   "source": [
    "## Configure SSL Training\n",
    "\n",
    "Set up training parameters for self-supervised pre-training:\n",
    "- **10 epochs** on the large CNN/DailyMail dataset\n",
    "- **Early stopping** to prevent overfitting\n",
    "- **Evaluation every 2000 steps** to monitor progress\n",
    "\n",
    "This teaches the model general language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tjqu0vn0rUY2",
    "outputId": "8556f93a-aedd-4846-e42c-a133bc112d92"
   },
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Load fresh model for SSL training\n",
    "ssl_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# SSL training configuration\n",
    "ssl_output_dir = f'/content/drive/MyDrive/Mdata_lab/Dataset/Self_Supervised_Learning/model/flan-t5-ssl-{str(int(time.time()))}'\n",
    "\n",
    "ssl_training_args = TrainingArguments(\n",
    "    output_dir=ssl_output_dir,\n",
    "    num_train_epochs=10,  # SSL pre-training\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500,\n",
    "    eval_steps=2000,\n",
    "    save_steps=2000,\n",
    "    eval_strategy=\"steps\",\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "ssl_trainer = Trainer(\n",
    "    model=ssl_model,\n",
    "    args=ssl_training_args,\n",
    "    train_dataset=ssl_datasets['train'],\n",
    "    eval_dataset=ssl_datasets['validation'],\n",
    ")\n",
    "\n",
    "print(\"Starting SSL pre-training (Denoising Autoencoder)...\")\n",
    "print(f\"Training on {len(ssl_datasets['train'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gRyu4EsV25S"
   },
   "source": [
    "## Execute SSL Pre-training\n",
    "\n",
    "**WARNING**: This cell takes several hours to complete!\n",
    "\n",
    "The model learns to:\n",
    "- Understand context and word relationships\n",
    "- Predict missing words from surrounding context\n",
    "- Build robust internal representations\n",
    "\n",
    "These learned representations will transfer to downstream tasks with minimal fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TW82JFQJwIwg"
   },
   "outputs": [],
   "source": [
    "# Train SSL model\n",
    "ssl_trainer.train()\n",
    "\n",
    "# Save SSL model\n",
    "ssl_trainer.save_model(ssl_output_dir)\n",
    "tokenizer.save_pretrained(ssl_output_dir)\n",
    "print(f\"\\nSSL model saved to: {ssl_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GovAvm-h5gfk"
   },
   "source": [
    "---\n",
    "\n",
    "# LoRA Fine-Tuning\n",
    "\n",
    "Now we'll take our SSL pre-trained model and fine-tune it using **LoRA** (Low-Rank Adaptation) on a small labeled dataset.\n",
    "\n",
    "### What is LoRA?\n",
    "**Low-Rank Adaptation** adds small trainable \"adapter\" matrices to frozen model weights. Instead of updating all 248M parameters, we train only ~4M adapter parameters.\n",
    "\n",
    "## Why LoRA?\n",
    "- **Parameter Efficient**: Only trains ~1% of model parameters\n",
    "- **Fast**: Significantly faster than full fine-tuning\n",
    "- **Memory Efficient**: Reduces GPU memory requirements\n",
    "- **Modular**: LoRA adapters can be swapped without retraining base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnb0MjmDWnfS"
   },
   "source": [
    "## Load SSL Model and Apply LoRA\n",
    "\n",
    "### LoRA Configuration:\n",
    "- **r=16**: Rank of adaptation matrices (higher = more capacity, more parameters)\n",
    "- **lora_alpha=32**: Scaling factor for LoRA updates\n",
    "- **target_modules**: Apply LoRA to attention query, key, value, and output projections\n",
    "\n",
    "The SSL-pretrained checkpoint provides our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Dwgs3m15fm3",
    "outputId": "451db4e3-fcfa-4e0c-91d3-41b1ea7fae9f"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "# Load SSL pre-trained model\n",
    "#print(f\"Loading SSL model from: {ssl_output_dir}\")\n",
    "#ssl_pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(ssl_output_dir, torch_dtype=torch.bfloat16)\n",
    "\n",
    "latest_checkpoint = '/content/drive/MyDrive/Mdata_lab/Dataset/Self_Supervised_Learning/model/flan-t5-ssl-1767904233/checkpoint-30000'\n",
    "\n",
    "# Load model from checkpoint\n",
    "ssl_pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    latest_checkpoint,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA scaling factor\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\"]  # Apply LoRA to query and value projections\n",
    ")\n",
    "\n",
    "# Apply PEFT to SSL model\n",
    "peft_model = get_peft_model(ssl_pretrained_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"\\n\" + print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhrPx6AQXW6q"
   },
   "source": [
    "## Load Target Task Dataset\n",
    "\n",
    "Now we load **DialogSum** - a smaller, high-quality dataset of conversations and summaries. This is our target task: dialogue summarization.\n",
    "\n",
    "**Key Point**: We use DialogSum for supervised fine-tuning with much less data than the SSL pre-training phase. The SSL learning transfers to this new task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484,
     "referenced_widgets": [
      "e68c6337f4624a9999ebb10fd41bc49a",
      "70c917d46fa54aa2b78367586b8eaa04",
      "3ec438e2f9054414bedb693897581edd",
      "48f761585f2540c4b3a88a9c5e99011c",
      "12212211265043ddaca41cd273c178f7",
      "e59a0c30f028486b8e4d0fa7ade70756",
      "49a967d221c14007a34cc5913f9c1fe7",
      "22383df61d534273a340f76e0b30069c",
      "eb9ce39845e3402080a77d14b81c4ea5",
      "a9e048bd74874e198b76f22187f0c2ab",
      "b71472ed9b9d415a8baf2222914a6035",
      "59041fdc1b254887971b8fd12ecfe160",
      "8f21409a038747e7a509cf4ec8a4d88a",
      "ccf02c648c86430db14641060de8270f",
      "88c2835a4f4a4c25a533ce1b65e6bf4c",
      "030ba89ffa1f485aa76ccefae381bf14",
      "6a9b4d8e75974b79b2f8f07c86627c5a",
      "2e4af6017b18499397eba1dce9608691",
      "518ff1d961f44ff1b77cf355ec40db28",
      "053836f103a64091947dae2e1babcb9d",
      "793a8b08a9db4ee184b67bfab36ee5c6",
      "b1cf2d19155e4e4d925c8b76c2889b04",
      "8f80fe4b426341cabd10fe7ce2dd7642",
      "7efb07c64e0b4518a04d9fdc85bcdb32",
      "c033503223744518bb9894a0fde9daa7",
      "26a377c13d6d43ccae9f8b12926e8084",
      "e858906767594e7ab9a2863526cb3c6f",
      "90a3e431f369497cb70ea41f834c9759",
      "bb6edf05e558473c822c0ca772580bdf",
      "6c522de0abb645e4a2f148922a7e33b3",
      "4eb3f3523bc5467ea4bda72b163fa580",
      "0fa52c989fd742598ee3f11e24f72864",
      "ccdab12c2aeb43a6b594ac865b02588c",
      "30a949f615634ccfaacc8f297053e073",
      "d7147ea78e4b444ea47f34bd73ba0408",
      "72b0e8e7d73746258c8b9d60754012a6",
      "2811b62f1a4346c0a20161b718232ef0",
      "8423381cc32d4d7d832c53a77bd954df",
      "caf25ff0e06a429b9720eadfeb78febd",
      "7324127eca45449188868492e4eae009",
      "d09b1451730247d9b09ee5770463e394",
      "11e3613cc34548a584a35880bbd37b1f",
      "6e3b121f84004c299dcf2592eeb25fc6",
      "0fc9a08da1e64558a87c63405cb5dd85",
      "0725b6cfadf5427483b944ec1fffd6e6",
      "2a15a6c92a9d4742af6d02ae9cca6080",
      "f610e8569c264c688732aa85edf8c978",
      "1c9d63b5aaeb48d2a2c05c1728ce4f48",
      "d2cf43c610dd47cf8202ca4ff340a225",
      "8d81578424ae4eccbc55bed4862af294",
      "7d494e7e005a4a4daa790efaeed96902",
      "2789f445222d4362b8c98a7e723fbf80",
      "6686c6f7cbbf425aaf6702bb095bdc6a",
      "573d20fae81643bfbd959b7df5a0ead5",
      "5dc0b5c63f04439aa497f0739e08fa8c",
      "7cac73a9cde34227bf7070e377afa0ef",
      "1967398699a643fdad95fef17e28d14b",
      "a5bcbcf7b27d4d258ccf6f81e843f150",
      "fe449860026349f7abc20b32f0c81b9e",
      "f553fadfbfee4dd68db7c0fdb47f0c10",
      "164c72e5b9d4404b8da04f172495b062",
      "707cb5ab56754c43bc1efd63e1db50ec",
      "486dc3277e254a8ca3045df0ff86ecdb",
      "7fb3fb2db306456d9dee5dfebb27ebd4",
      "f21c4d240a274dec896a0eb221e29608",
      "989935c1e3aa465eaaa582220b5600f6",
      "6b1061f1b82d400da66f4d18ebdd3260",
      "8741128ce630447c9a2cdb51d0fbd8f3",
      "40a88cb2444d440d928bcaf57864cfcc",
      "12153f961b374280a6764682d8d14cef",
      "f182e784d9174684bc21f1c75bcb079c",
      "6da7f72ba5b54f6ebc0e4e410592faf2",
      "95381854ece543448eb7c783294cc245",
      "e7dbde0fde64440e971f1df86b54ce48",
      "aa7073ede64e4f6b9d8cc3dd7fb9ca95",
      "626eaa4a9b454ad5a97fe9f4a294b8ab",
      "2232d3d8f0944fa6a8a14678a6e30c49"
     ]
    },
    "id": "RbXQhD6c8OsO",
    "outputId": "c33cb1fa-8ad4-4d1a-b2a6-5467e9f532f4"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"knkarthick/dialogsum\" #loading the dataset from huggingface\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqNOsC9wXp2m"
   },
   "source": [
    "## Tokenize Supervised Learning Data\n",
    "\n",
    "Convert dialogue-summary pairs into a format the model can learn from:\n",
    "- **Input**: \"Summarize the following conversation.\\n\\n[dialogue]\\n\\nSummary: \"\n",
    "- **Target**: The actual summary\n",
    "\n",
    "This is **supervised learning** - we have explicit input-output pairs with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "3e2c8da1c775457f8292c0ad9bdfe753",
      "df7dab4ed87e4773aff54b9e8e5a5f9a",
      "87c8f8d0fb8d4a46b074bc668d1e81b8",
      "5d9afc456dc7420d859d4955c6d70b5c",
      "194c6eea132a44efb3745a6a02effc17",
      "13739f8db5534256bd208ea2934e123c",
      "f575a7688fae4dcb8f20187821372821",
      "8b34012a1d7940d683b9e561f8f36fea",
      "e6ec2646dc314497be972fa94348ea20",
      "bfbbfc1a86a44704b45da6e57288d3de",
      "baf6cc013c974a13b03cc78ac510b6f9",
      "d48df3cec184453baa7e3d3dd5dea2a7",
      "b7c875a2bd5547c39cec95d23510a346",
      "5bed0697661e4e1c87c3ac37947ec4c1",
      "2246bfdd0f3a46c997297ed3cc7e190c",
      "2f55dc9330624546b98b04f66c8137eb",
      "ab310fad3cc74cb3bd73ed7b40f6cb8a",
      "8a242daa456547f6ac176f34b0c7a076",
      "48fe0cf068bf445c9624fdd7f0c14796",
      "d8d575614e8f41c4876855a67df49dde",
      "f2d3438dae734fe5885582f2d7f1d2e2",
      "42532acf9d434505a00849019a926d22",
      "4adbbde5f87a409e9bb8281f72cd8dad",
      "838062298708441691060a91c0807517",
      "1b6874af2faf48788eac032cb2b5f20f",
      "b0844143a93148cdae0208ada7c16328",
      "3b5229b5406b4514ac1143a30030ef2e",
      "3dfa8ff4b5984c1b9ab05516dc839e9a",
      "80c0e353c4824543aaf0e55c2f52f3b8",
      "ad13077f4d0f4285bfdb2dfb50c43b22",
      "303f4e212d9e4fcfb5e5a49ac30ca3b6",
      "ae9fb99b25f34454907aed027c605559",
      "857f7fa64ed947c3aacf76288331f475"
     ]
    },
    "id": "V9tpdGg57Xxs",
    "outputId": "e84717f2-0a98-43c4-8c07-066a48fe73eb"
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset by converting the dialogue-summary (prompt-reponse) pairs into explicit instructions\n",
    "# Then convert the dialogue-response into tokens\n",
    "\n",
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = original_tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = original_tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DOcN7A17jng",
    "outputId": "a2e6b83f-1282-4c9d-f383-0abcb00c3f26"
   },
   "outputs": [],
   "source": [
    "# Shape of the tokenized datasets\n",
    "\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgNPN0K5YVzK"
   },
   "source": [
    "## Create Small Labeled Subset\n",
    "\n",
    "**Critical Experiment Design**: We use only **50% of the training data** to demonstrate that SSL pre-training enables strong performance with limited labeled data. You can experiment with fewer samples.\n",
    "\n",
    "**Hypothesis**: SSL pre-training provides general language understanding, so we need less task-specific labeled data for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGeC7ZSb6I8F",
    "outputId": "75000722-9fd2-46db-ced2-528962ef3d8f"
   },
   "outputs": [],
   "source": [
    "# Use only 50% of training data for supervised fine-tuning\n",
    "subset_size = len(tokenized_datasets['train']) // 2\n",
    "small_train = tokenized_datasets['train'].select(range(subset_size))\n",
    "\n",
    "print(f\"Using {len(small_train)} examples for supervised fine-tuning (50% of training data)\")\n",
    "print(f\"Full training set: {len(tokenized_datasets['train'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikaL98pNY1K0"
   },
   "source": [
    "## Configure LoRA Fine-Tuning\n",
    "\n",
    "Setup training for the PEFT (Parameter-Efficient Fine-Tuning) phase:\n",
    "- **10 epochs** with early stopping\n",
    "- **Higher learning rate** (1e-4) because we're only training adapter layers\n",
    "- **Small batch size** (16) to fit in memory\n",
    "- Only the LoRA adapter weights are updated, base model stays frozen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUZGwU3Y8LW8",
    "outputId": "2d7bcd88-e7af-40f3-ed18-13cdeee1be6f"
   },
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# PEFT training configuration\n",
    "peft_output_dir = f'/content/drive/MyDrive/Mdata_lab/Dataset/Self_Supervised_Learning/model/flan-t5-peft-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=peft_output_dir,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,  # Higher learning rate for adapter layers\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=small_train,  # Small labeled subset\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")\n",
    "\n",
    "print(\"Starting PEFT fine-tuning on summarization task...\")\n",
    "print(f\"Only LoRA adapters will be trained, base model is frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU8M6i7DZEnT"
   },
   "source": [
    "## Train LoRA Adapters\n",
    "\n",
    "This should be much faster than SSL pre-training since we're:\n",
    "1. Training fewer parameters (only LoRA adapters)\n",
    "2. Using a smaller dataset\n",
    "3. Building on SSL-learned representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "id": "tOzETQiT83g5",
    "outputId": "480a29a1-1a17-4347-c1fe-94768d39997a"
   },
   "outputs": [],
   "source": [
    "# Train PEFT model\n",
    "peft_trainer.train()\n",
    "\n",
    "# Save PEFT model\n",
    "peft_trainer.save_model(peft_output_dir)\n",
    "original_tokenizer.save_pretrained(peft_output_dir)\n",
    "print(f\"\\nPEFT model saved to: {peft_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "ft5mws_5LJHU",
    "outputId": "54892178-3cde-42be-f07a-ac6dbcd8828e"
   },
   "outputs": [],
   "source": [
    "# Load the latest checkpoint to resume training\n",
    "#latest_checkpoint = '/content/drive/MyDrive/Mdata_lab/Dataset/Self_Supervised_Learning/model/flan-t5-peft-1768103269/checkpoint-49500'\n",
    "\n",
    "# resume training\n",
    "#peft_trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "#original_tokenizer.save_pretrained(peft_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUS5Sujq_MDH"
   },
   "source": [
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "Now we'll evaluate our SSL+LoRA approach against the baseline model using standard summarization metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4f5JqUEZq2_"
   },
   "source": [
    "## Load Final Trained Model\n",
    "\n",
    "Load the best checkpoint from our PEFT fine-tuning for evaluation. This model combines:\n",
    "1. FLAN-T5 base weights\n",
    "2. SSL-learned representations\n",
    "3. LoRA adapters fine-tuned on DialogSum dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZGkhk3Snevb"
   },
   "outputs": [],
   "source": [
    "peft= '/content/drive/MyDrive/Mdata_lab/Dataset/Self_Supervised_Learning/model/flan-t5-peft-1768318334/checkpoint-3600'\n",
    "peft_tokenizer = '/content/drive/MyDrive/Mdata_lab/Dataset/Self_Supervised_Learning/model/flan-t5-peft-1768318334'\n",
    "peft_model = AutoModelForSeq2SeqLM.from_pretrained(peft, dtype=torch.bfloat16)\n",
    "peft_tokenizer = AutoTokenizer.from_pretrained(peft_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qf700kJj54r"
   },
   "source": [
    "## Metrics\n",
    "- **ROUGE-1**: Unigram (1-word) overlap between generated and reference summary\n",
    "- **ROUGE-2**: Bigram (2-word) overlap - captures phrase similarity\n",
    "- **ROUGE-L**: Longest common subsequence - measures fluency\n",
    "- **BLEU**: Precision-focused metric from machine translation\n",
    "- **BERTScore**: Semantic similarity using BERT embeddings - captures meaning even with different words\n",
    "\n",
    "**Note**: **ROUGE-L** and **BERTScore** are the main evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEDr8Fq4j6mu"
   },
   "outputs": [],
   "source": [
    "%pip install evaluate rouge-score bert-score\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "58577d60abe14d7ba562e085535cea0c",
      "57abb0d62d82447e8ed10f1aba7c1410",
      "e8c1e2a307a7403ab7d1650791ae9125",
      "9894620351054648957eaa3b3a4d166f",
      "f8512e31c77047c89e7ea5df723309b4",
      "883a3b969f44459eae6a6adaa772f474",
      "4156ff38bd044fb4bbff9146428b17df",
      "24d906b470fb4129a06d7b56311c78e0",
      "30f7c6e0628d47448d2d22a732ad4f7d",
      "b7e7badd9f644325945956ea308874ce",
      "aa3edb44f5ed486e887531e80f66d4b1",
      "e9731aec5a184b1c85d8df179e9ab44f",
      "392f8ce7e40a4212aedb808c34e92d4c",
      "27a3866a6502417e9819bdb2f9e241cf",
      "aed6312c9dcf4fcfa495750635054377",
      "e8f5bd2b121d4606a33240760ec174ca",
      "bf847b7e2c0a41d298f4b10fcd7759db",
      "60db8d20dde44751864ada11d4d5f8d6",
      "08d13ee2a89846cd9aed8a6ef77a6ab1",
      "73955b37f3714c729b07af6aeedc9acc",
      "039034dd3ce94801b4f9d124ebccd66d",
      "1230b7fc4ec544da8d6d8c11e0fb9775",
      "70e4691a16d040bc971aa35a802ef45c",
      "edad1da021ce40fdab1fa0298e88b22b",
      "c549142cf0be49f08dfe8bcedb02187e",
      "0616978b16de4f00a9db48035ab3079e",
      "5519813896784d08a1a7913c4cea43f7",
      "c5347bf4f17f4c5a8cb56fef8b4467ba",
      "eb10470871f544efbd7bd34ced561219",
      "3b74c485616843c1b68f7970ddb8eeee",
      "d80b232533194a68a7cd4d7573b5e38e",
      "4a22fa014efd469eab472b3eb05f1796",
      "c35bd1bab04140be95b63818b1a1966d",
      "8836ef751cd5468493912e1d7ff36f53",
      "64af6c51c6ac4653b4c7b14c930139ec",
      "7e8664bcd908475f8c8cc55468a31c47",
      "5b18ab849e4744f9badb929b77d3447c",
      "8b2fe783a75846af808641635bca24c1",
      "8633e3aab6dd465b8d34be4f6c8a66dd",
      "4abb7152b9374f8b9809fcfe709cb02a",
      "d873460c47274e9ab9fa31f9ad82cccf",
      "ca42ec60985a4a6896e1f089c8d50012",
      "5d7a96d6117d4c53b5d05ea4ec10d0c8",
      "5395c019fbbd4abf931c097ce855d6a6",
      "9b504e9f6b9d44898ba0921c03c20394",
      "04189036b8d942b493f7a785fe419b94",
      "f4d719888ea143abb08b167180e4f105",
      "dcad05ead3e64833ba8971220446ea70",
      "ab9ad5723cd3430da90d2be71f6906d7",
      "f80faca0e314444c9aff69396fd67af5",
      "19d6f5d48f5e4d4e88365a1d1b6672ae",
      "e47cb135adce4b789ce31a53354ac01f",
      "d41ab6ccacc24746b560730aeec97ef4",
      "a9391437a2394a3ab2fe386a5cd07b4d",
      "305cd1369bcd48058807af2b28181542"
     ]
    },
    "id": "yoR4wMPDj9-G",
    "outputId": "03d824f8-c3b3-4a18-bac8-ef2fcc1e946d"
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "bertscore = evaluate.load('bertscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPOJXruRkfBn",
    "outputId": "9936dd36-c1a8-4a8a-d5e9-3b3ffe9c1e55"
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_summaries(predictions, references, method_name):\n",
    "    \"\"\"Evaluate predictions against references using multiple metrics\"\"\"\n",
    "\n",
    "    # ROUGE scores\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # BLEU score\n",
    "    bleu_results = bleu.compute(\n",
    "        predictions=predictions,\n",
    "        references=references\n",
    "    )\n",
    "\n",
    "    # BERTScore (semantic similarity)\n",
    "    bert_results = bertscore.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        lang='en',\n",
    "        model_type='distilbert-base-uncased'  # Faster variant\n",
    "    )\n",
    "\n",
    "    # Calculate average BERTScore\n",
    "    avg_bertscore = sum(bert_results['f1']) / len(bert_results['f1'])\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Results for: {method_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"ROUGE-1:      {rouge_results['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2:      {rouge_results['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L:      {rouge_results['rougeL']:.4f}\")\n",
    "    print(f\"BLEU:         {bleu_results['bleu']:.4f}\")\n",
    "    print(f\"BERTScore: {avg_bertscore:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'Method': method_name,\n",
    "        'ROUGE-1': round(rouge_results['rouge1'], 4),\n",
    "        'ROUGE-2': round(rouge_results['rouge2'], 4),\n",
    "        'ROUGE-L': round(rouge_results['rougeL'], 4),\n",
    "        'BLEU': round(bleu_results['bleu'], 4),\n",
    "        'BERTScore': round(avg_bertscore, 4)\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2QlAkE-bRnB"
   },
   "source": [
    "## Generate Predictions for Comparison\n",
    "\n",
    "Generate summaries from both models on 10 randomly selected test examples:\n",
    "1. **Original Model**: FLAN-T5 with no additional training\n",
    "2. **PEFT Model**: FLAN-T5 + SSL pre-training + LoRA fine-tuning\n",
    "\n",
    "This allows us to directly compare the impact of our SSL+LoRA approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6wPzdB8koce",
    "outputId": "580ccf58-2796-4b01-aa98-4a2ffb7ad419"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for all methods on test samples\n",
    "#test_indices = [1, 55, 80, 150]\n",
    "import random\n",
    "\n",
    "# Randomly select 10 samples from the test set\n",
    "random.seed(48)  #\n",
    "test_indices = random.sample(range(len(dataset['test'])), 10)\n",
    "\n",
    "# Storage for predictions\n",
    "baseline_summaries = []\n",
    "original_predictions = []\n",
    "#few_shot_predictions = []\n",
    "peft_predictions = []\n",
    "\n",
    "print(\"Generating predictions for all methods...\")\n",
    "print(f\"Testing on {len(test_indices)} samples\\n\")\n",
    "\n",
    "for idx, index in enumerate(test_indices):\n",
    "    article = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    # Store baseline\n",
    "    baseline_summaries.append(summary)\n",
    "\n",
    "    print(f\"Processing sample {idx + 1}/{len(test_indices)}...\")\n",
    "\n",
    "    # Original prediction\n",
    "    zero_shot_prompt = f\"Summarize the following conversation.\\n\\n{article}\\n\\nSummary: \"\n",
    "    inputs = original_tokenizer(zero_shot_prompt, return_tensors='pt').to(original_model.device)\n",
    "    output = original_tokenizer.decode(\n",
    "        original_model.generate(inputs[\"input_ids\"], max_new_tokens=2000, do_sample=False)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    original_predictions.append(output)\n",
    "\n",
    "    # PEFT prediction\n",
    "    peft_prompt = f\"Summarize the following conversation.\\n\\n{article}\\n\\nSummary: \"\n",
    "    inputs = peft_tokenizer(peft_prompt, return_tensors='pt')\n",
    "    inputs = {k: v.to(peft_model.device) for k, v in inputs.items()}\n",
    "    output = peft_tokenizer.decode(\n",
    "        peft_model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=2000, do_sample=False)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    peft_predictions.append(output)\n",
    "\n",
    "print(f\"\\n Generated {len(test_indices)} predictions for each method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541,
     "referenced_widgets": [
      "d60cbaa1529247c0845e7ec2cb999f15",
      "f53fb894b4494dd78d6ee44ad546b1ee",
      "0087902f2e1f43a89b2efd050eedb380",
      "c17ee35410a44c97a9e127c227f9b0db",
      "af0f8c48309b48af9f6c59949ab97db7",
      "42979940be9d47e682bbb3a5c28da2cc",
      "2788f26e972a4d668d2ed09d80d39143",
      "761a606ca74c4fc3a1f05b2a3960ebdb",
      "cc23bb8c31184dc4a1250336a564b9c6",
      "943ec021f4664482beb073b4b64895e4",
      "b2238e8f9428453f9d22fb67fdac9b03",
      "cab4320e09784b428b905f141ee418a0",
      "9a60b4e395874965be7773453b916005",
      "2b9bfa716ff44b2789788b4ec0f58717",
      "9b16ee6b65b44422a65b40140f11b257",
      "6233562cf4bf47e69f63c3fad99dfbc2",
      "3f0e168291cb4cfca5528efea510d463",
      "65dd1ed161d342ada8db8729fc0aa91d",
      "73bec82ffdfa4666a08f17bba0150978",
      "b7dc2e5fcbeb40c491df4d9ce2680875",
      "cc40990aa4cc4c379558ee1b1b3d3887",
      "babae9aa5f84491abea5a5913e55025f",
      "6d8f729f4a3240cabbe3e50510d21736",
      "7a20ee36bd914a1fbdff7bd9f076ef97",
      "5bc2060cf4d741e09c365bcbbb427728",
      "9e03f097267144a99cf0bb84d73fe31c",
      "d1cabfa8224942ce903abcf2cb0e8f86",
      "f7493448084046c5b8cbaf2c3709e04d",
      "395d0da8542d457f929030795c0fba9b",
      "a86fc3cc5a7b4678b23779d5e24914d5",
      "2f4b49e271934f5d988e3ee82e05c5f4",
      "5833ed9c3125481481d36133c07a052d",
      "aa8ee4cef85b4cf1bc1a98de9fd9c9a8",
      "2f97198fd2f6423e8e21c261d40d0257",
      "92ebe73be7ae4e41878c1c806407e8db",
      "7b88f331d39044a6bbb37fd00bf7a55b",
      "dcdef9dd259a4519bc440a825c56eadd",
      "2252ce7491a64e5597038d1fa88ac2ef",
      "b5321083d09942cd936658e3525be897",
      "9595d851dc744f2ca66ce402a36bab4e",
      "2f2b732b7ee34434b1f44d13b2675d33",
      "8d17197bdde447a081264fb1146e1690",
      "899c54495c8943b1bbd030fe164952c4",
      "6375cf6c8b4f4115a4c660689ccdcaae",
      "33a6b1f804794aafb53ac203dac32673",
      "05851bcc7ddb4bf1a2110aa1718174e5",
      "5ea90befbc6342eebcd423f883bec0a5",
      "2c94a28aea5343b08037811fd0356594",
      "8b649ff2b1d1458ca599208c2833f7fb",
      "09a7e5d3fdca4e75b51eb892195854bb",
      "7fccea2193df4169b6b1f086ada52c58",
      "0740589b6db74e549cb5b110bdb3ed84",
      "a59e7e16755047b8bdec0858735d8602",
      "ba90e5bc650844bebca486160f2a1515",
      "190e3b9cac9f40658369ba6a8773545d"
     ]
    },
    "id": "deDYk9nLlUO4",
    "outputId": "666fd04e-0a4c-452c-bf83-7a06cb14c297"
   },
   "outputs": [],
   "source": [
    "# Evaluate all methods\n",
    "results_list = []\n",
    "\n",
    "print(\"\\nEvaluating all methods against baseline summaries...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate Zero-Shot\n",
    "results_list.append(evaluate_summaries(original_predictions, baseline_summaries, \"Original Model\"))\n",
    "\n",
    "# Evaluate PEFT (SSL + LoRA)\n",
    "results_list.append(evaluate_summaries(peft_predictions, baseline_summaries, \"PEFT (SSL+LoRA)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxKVxpQMbsz-"
   },
   "source": [
    "The PEFT model show significant improvement across all metrics, demonstrating that:\n",
    "1. SSL pre-training provides useful representations\n",
    "2. LoRA enables effective fine-tuning with limited labeled data\n",
    "3. The combined approach is more data-efficient than training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20NlRJlpbx7F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
